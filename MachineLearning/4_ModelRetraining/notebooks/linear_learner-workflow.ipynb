{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to your Amazon SageMaker notebook instance!  \n",
    "\n",
    "This is a fully managed AWS environment that provides you a Jupyter Notebook to work with data.  To learn more about Amazon SageMake notebook instances, check out our [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html).\n",
    "\n",
    "## Summary\n",
    "\n",
    "We're looking to build our machine learning workflow that will retrain our model when new data sets are added.  Instead of executing python code inside of a Notebook, we'd like to execute these steps in a state machine. We will leverage the [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io/) to create workflow. The steps below are still performed:\n",
    "\n",
    "1. Setup serverless querying of data in S3 via [Amazon Athena](https://aws.amazon.com/athena/).\n",
    "2. Prepare dataframes using [pandas](https://pandas.pydata.org/) and [numpy](https://numpy.org).\n",
    "3. Build and train a machine learning model via the [Amazon SageMaker Python SDK](https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html).\n",
    "\n",
    "But instead across these states:\n",
    "\n",
    "1. Prep data in Athena (Lambda function)\n",
    "2. Split data into training and testing sets (Lambda function)\n",
    "3. Model training (Step functions data science SDK)\n",
    "4. Save the model (Step functions data science SDK)\n",
    "5. Check model accuracy (Lambda function)\n",
    "6. Test: Accuracy above our threshold?\n",
    "7. Yes? Publish\n",
    "8. No? Do nothing\n",
    "\n",
    "To get started, let's input the name of the S3 bucket you created earlier in this workshop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTION: provide the data bucket NAME you are using for this workshop\n",
    "data_bucket = '' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we'll need to install and load all the required modules. Then we'll create an IAM role for the Step Functions resources that we will create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade stepfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "import stepfunctions\n",
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "import json\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.steps import TrainingStep, ModelStep\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "session = sagemaker.Session()\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "bucket = session.default_bucket()\n",
    "id = uuid.uuid4().hex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add permissions to your notebook role in IAM\n",
    "\n",
    "The IAM role assumed by your notebook requires permission to create and run workflows in AWS Step Functions. If this notebook is running on a SageMaker notebook instance, do the following to provide IAM permissions to the notebook:\n",
    "\n",
    "1. Open the Amazon  [SageMaker console](https://console.aws.amazon.com/sagemaker/).\n",
    "1. Select **Notebook instances** and choose the name of your notebook instance.\n",
    "1. Under **Permissions and encryption** select the role ARN to view the role on the IAM console.\n",
    "1. Copy and save the IAM role ARN for later use.\n",
    "1. Choose **Attach policies** and search for `AWSStepFunctionsFullAccess`.\n",
    "1. Select the check box next to `AWSStepFunctionsFullAccess` and choose **Attach policy**.\n",
    "\n",
    "Next, let's create an execution role in IAM for Step Functions.\n",
    "\n",
    "### Create an Execution Role for Step Functions\n",
    "\n",
    "Your Step Functions workflow requires an IAM role to interact with other services in your AWS environment.\n",
    "\n",
    "1. Go to the [IAM console](https://console.aws.amazon.com/iam/).\n",
    "1. Select **Roles** and then **Create role**.\n",
    "1. Under **Choose the service that will use this role** select **Step Functions**.\n",
    "1. Choose **Next** until you can enter a **Role name**.\n",
    "1. Enter a name such as `StepFunctionsWorkflowExecutionRole` and then select **Create role**.\n",
    "\n",
    "Next, create and attach a policy to the role you created. As a best practice, the following steps will attach a policy that only provides access to the specific resources and actions needed for this solution.\n",
    "\n",
    "1. Under the **Permissions** tab, click **Attach policies** and then **Create policy**.\n",
    "1. Enter the following in the **JSON** tab:\n",
    "    ```json\n",
    "    {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": \"iam:PassRole\",\n",
    "                \"Resource\": \"NOTEBOOK_ROLE_ARN\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"sagemaker:CreateModel\",\n",
    "                    \"sagemaker:DeleteEndpointConfig\",\n",
    "                    \"sagemaker:DescribeTrainingJob\",\n",
    "                    \"sagemaker:CreateEndpoint\",\n",
    "                    \"sagemaker:StopTrainingJob\",\n",
    "                    \"sagemaker:CreateTrainingJob\",\n",
    "                    \"sagemaker:UpdateEndpoint\",\n",
    "                    \"sagemaker:CreateEndpointConfig\",\n",
    "                    \"sagemaker:DeleteEndpoint\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    \"arn:aws:sagemaker:*:*:*\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"events:DescribeRule\",\n",
    "                    \"events:PutRule\",\n",
    "                    \"events:PutTargets\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTrainingJobsRule\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"lambda:InvokeFunction\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    \"arn:aws:lambda:*:*:function:*QueryTraining*\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    ```\n",
    "1. Replace **NOTEBOOK_ROLE_ARN** with the ARN for your notebook that you created in the previous step.\n",
    "1. Choose **Review policy** and give the policy a name such as `StepFunctionsWorkflowExecutionPolicy`.\n",
    "1. Choose **Create policy**.\n",
    "1. Select **Roles** and search for your `StepFunctionsWorkflowExecutionRole` role.\n",
    "1. Under the **Permissions** tab, click **Attach policies**.\n",
    "1. Search for your newly created `StepFunctionsWorkflowExecutionPolicy` policy and select the check box next to it.\n",
    "1. Choose **Attach policy**. You will then be redirected to the details page for the role.\n",
    "1. Copy the `StepFunctionsWorkflowExecutionRole` **Role ARN** at the top of the Summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the permissions that the Step Function Workflow will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste the StepFunctionsWorkflowExecutionRole ARN from above\n",
    "workflow_execution_role = ''\n",
    "\n",
    "# SageMaker Execution Role\n",
    "# You can use sagemaker.get_execution_role() if running inside sagemaker's notebook instance\n",
    "sagemaker_execution_role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the specific S3 bucket name that we will grant permissions to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the locations where the training data will be placed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prefix = 'train'\n",
    "test_prefix = 'test'\n",
    "key = 'recordio-pb-data'\n",
    "\n",
    "train_data = 's3://{}/{}/'.format(data_bucket, train_prefix)\n",
    "test_data = 's3://{}/{}/'.format(data_bucket, test_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the API for your Step Function Workflow\n",
    "This is the API that controls the inputs of your State Machine.\n",
    "\n",
    "* **TrainingJobName** = The name of your SageMaker Training Job\n",
    "* **LambdaDataPrep** = The name of the Lambda function responsible for data prep\n",
    "* **LambdaDataSplit** = The name of the Lambda function responsible for splitting the data\n",
    "* **LambdaDeployModel** = The name of the Lambda function responsible for deploying the model\n",
    "* **ModelName** = The name of the model we'll be training\n",
    "* **EndpointName** = The name of the Endpoint (unused)\n",
    "* **LambdaQueryStatus** = The name of the Lambda function responsible for querying the model accuracy\n",
    "* **data_bucket** = The name of the S3 bucket where all of the data for the workflow will be persisted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker expects unique names for each job, model and endpoint. \n",
    "# If these names are not unique the execution will fail.\n",
    "execution_input = ExecutionInput(schema={\n",
    "    'TrainingJobName': str,\n",
    "    'LambdaDataPrep': str,\n",
    "    'LambdaDataSplit': str,\n",
    "    'LambdaDeployModel': str,\n",
    "    'ModelName': str,\n",
    "    'EndpointName': str,\n",
    "    'LambdaQueryStatus': str,\n",
    "    'data_bucket': str\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the SageMaker Estimator for training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "\n",
    "def estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=None):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, \n",
    "    and return a deployed predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    linear = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"linear-learner\"),\n",
    "        get_execution_role(),\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.m5.2xlarge',\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session())\n",
    "    linear.set_hyperparameters(**hyperparams)\n",
    "    return linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# TODO: Update hard coded feature_dim and mini_batch_size params to be dynamic again (refer to old code)\n",
    "hyperparams = {\n",
    "    'feature_dim': 6,\n",
    "    'mini_batch_size': 17995,\n",
    "    'predictor_type': 'binary_classifier' \n",
    "}\n",
    "\n",
    "output_path = 's3://' + data_bucket\n",
    "linear_estimator = estimator_from_hyperparams(train_data, hyperparams, output_path, \n",
    "                                                   s3_test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Step Function Training Step\n",
    "\n",
    "The next thing we'll do is define the training step and pass the estimator we defined above. See [TrainingStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep) in the documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_data = 's3://{}/{}/{}'.format(data_bucket, train_prefix, key)\n",
    "s3_test_data = 's3://{}/{}/{}'.format(data_bucket, test_prefix, key)\n",
    "\n",
    "training_step = steps.TrainingStep(\n",
    "    'Model Training', \n",
    "    estimator=linear_estimator,\n",
    "    data={\n",
    "        'train': s3_train_data,\n",
    "        'test': s3_test_data\n",
    "    },\n",
    "    job_name=execution_input['TrainingJobName'],\n",
    "    wait_for_completion=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Step Function Model Save Step\n",
    "\n",
    "Let's define a model step that will create a model in SageMaker using the artifacts created during the TrainingStep. See [ModelStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.ModelStep) in the  documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = steps.ModelStep(\n",
    "    'Save Model',\n",
    "    model=training_step.get_expected_model(),\n",
    "    model_name=execution_input['ModelName'],\n",
    "    result_path='$.ModelStepResults'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Lambda Step Function Steps\n",
    "\n",
    "Let's define a lambda step for each one of the lambda functions that we will invoke as part of our Step Functions workflow. See [LambdaStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/compute.html#stepfunctions.steps.compute.LambdaStep) in the documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query lambda\n",
    "lambda_step_query = steps.compute.LambdaStep(\n",
    "    'Query Training Results',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['LambdaQueryStatus'],\n",
    "        'Payload':{\n",
    "            \"TrainingJobName.$\": '$.TrainingJobName'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# data prep lambda\n",
    "lambda_step_prep = steps.compute.LambdaStep(\n",
    "    'Prep Data in Athena',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['LambdaDataPrep'],\n",
    "        'Payload':{\n",
    "            \"data_bucket\": execution_input['data_bucket']\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# data split lambda\n",
    "lambda_step_split = steps.compute.LambdaStep(\n",
    "    'Split Data',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['LambdaDataSplit'],\n",
    "        'Payload': lambda_step_prep.output()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Choice State Step\n",
    "\n",
    "In order to build a dynamic workflow, we need to create a choice step. This choice step branches based off of the results of our training step: did the training job fail or should the model be deployed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy_step = steps.states.Choice(\n",
    "    'Accuracy > 90%'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Lambda Step Function Steps\n",
    "\n",
    "Let's define a lambda step for the lambda function that we will invoke if the accuracy of our model is higher than the defined threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moves the trained model to the \"deploy\" area for lambda to pick up\n",
    "lambda_model_deploy = steps.compute.LambdaStep(\n",
    "    'Deploy Model',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['LambdaDeployModel'],\n",
    "        'Payload':{\n",
    "            \"data_bucket\": execution_input['data_bucket'],\n",
    "            \"model_location\": execution_input['TrainingJobName']\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Fail State Step\n",
    "\n",
    "Let's define a Fail step which proceeds from our choice state if the accuracy of our model is lower than the defined threshold. See [FailStateStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/states.html#stepfunctions.steps.states.Fail) in the documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_step = steps.states.Fail(\n",
    "    'Model Accuracy Too Low',\n",
    "    comment='Validation accuracy lower than threshold'\n",
    ")\n",
    "\n",
    "end_step = steps.states.Pass('End')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Rules to Choice State\n",
    "\n",
    "The next thing we'll do is add a threshold rule to our choice state. If the validation accuracy of our model is below 0.90, we move to the fail state step. If the validation accuracy of our model is above 0.90, we move to the lambda model deployment step.\n",
    "\n",
    "To achieve an accuracy of 90%, we need error <.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_rule = steps.choice_rule.ChoiceRule.NumericLessThan(variable=lambda_step_query.output()['Payload']['trainingMetrics'][4]['Value'], value=.90)\n",
    "\n",
    "check_accuracy_step.add_choice(rule=threshold_rule, next_step=lambda_model_deploy)\n",
    "check_accuracy_step.default_choice(next_step=fail_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link all the Steps Together\n",
    "\n",
    "Finally, let's create a workflow definition by chaining all of the steps together that we've created. See [Chain](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.states.Chain) in the documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_definition = steps.Chain([\n",
    "    lambda_step_prep,\n",
    "    lambda_step_split,\n",
    "    training_step,\n",
    "    model_step,\n",
    "    lambda_step_query,\n",
    "    check_accuracy_step\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the workflow\n",
    "\n",
    "Now that we have the workflow definition, let's create the workflow and render the graph with render_graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_name = 'MLOpsRetrainingWorkflow_{}'.format(id)\n",
    "workflow = Workflow(\n",
    "    name=workflow_name,\n",
    "    definition=workflow_definition,\n",
    "    role=workflow_execution_role,\n",
    "    execution_input=execution_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.render_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the workflow in AWS Step Functions with `create`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_machines = Workflow.list_workflows()\n",
    "workflow_exist = False\n",
    "existing_workflow_obj = {}\n",
    "for machine in state_machines:\n",
    "    if machine[\"name\"] == workflow_name:\n",
    "        workflow_exist = True\n",
    "        existing_workflow_obj = machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if workflow_exist:\n",
    "    workflow = Workflow.attach(existing_workflow_obj[\"stateMachineArn\"])\n",
    "    print(workflow_name + \" already exist.  Updating the workflow definition.\")\n",
    "    workflow.update(definition=workflow_definition,role=workflow_execution_role)\n",
    "    # small delay for step functions to pick up the new definitions before execution\n",
    "    time.sleep(2)\n",
    "else:\n",
    "    print(workflow_name + \" does not exist.  Creating it with the specified workflow definition.\")\n",
    "    workflow.create()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the workflow with `execute`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a58fdf0d-32fb-4690-add3-433cc721773d"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "execution = workflow.execute(\n",
    "    inputs={\n",
    "        'TrainingJobName': 'linear-learner-{}'.format(id), # each Sagemaker Job requires a unique name,\n",
    "        'ModelName': 'UnicornWeatherImpact-{}'.format(id), # each Model requires a unique name,\n",
    "        'EndpointName': 'UnicornStatus', # each Endpoint requires a unique name\n",
    "        'LambdaQueryStatus': '',\n",
    "        'LambdaDataPrep': '',\n",
    "        'LambdaDataSplit': '',\n",
    "        'LambdaDeployModel': '',\n",
    "        'data_bucket': data_bucket\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render the workflow progress with the `render_progress`\n",
    "\n",
    "This generates a snapshot of the current state of your workflow as it executes. This is a static image therefore you must run the cell again to check progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to your Amazon SageMaker notebook instance!  \n",
    "\n",
    "This is a fully managed AWS environment that provides you a Jupyter Notebook to work with data.  To learn more about Amazon SageMake notebook instances, check out our [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html).\n",
    "\n",
    "## Summary\n",
    "\n",
    "We're looking to build our machine learning workflow that will retrain our model when new data sets are added.  Instead of executing python code inside of a Notebook, we'd like to execute these steps in a state machine.  The steps below are still performed:\n",
    "\n",
    "1. Setup serverless querying of data in S3 via [Amazon Athena](https://aws.amazon.com/athena/).\n",
    "2. Prepare dataframes using [pandas](https://pandas.pydata.org/) and [numpy](https://numpy.org).\n",
    "3. Build and train a machline learning model via the [Amazon SageMaker Python SDK](https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html).\n",
    "\n",
    "But instead across these states:\n",
    "\n",
    "1. Prep data in Athena (Lambda function)\n",
    "2. Split data into training and testing sets (Lambda function)\n",
    "3. Model training (Step function data science SDK)\n",
    "4. Save the model (Step function data science SDK)\n",
    "5. Check model accuracy (Lambda function)\n",
    "6. Test: Accuracy above our threshold?\n",
    "7. Yes? Publish\n",
    "8. No? Do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTION: provide the data bucket NAME you are using for this workshop\n",
    "data_bucket = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade stepfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "import stepfunctions\n",
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "import json\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker import s3_input\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.steps import TrainingStep, ModelStep\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "session = sagemaker.Session()\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "bucket = session.default_bucket()\n",
    "id = uuid.uuid4().hex\n",
    "function_name = 'query-training-status-{}'.format(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add permissions to your notebook role in IAM\n",
    "\n",
    "The IAM role assumed by your notebook requires permission to create and run workflows in AWS Step Functions. If this notebook is running on a SageMaker notebook instance, do the following to provide IAM permissions to the notebook:\n",
    "\n",
    "1. Open the Amazon  [SageMaker console](https://console.aws.amazon.com/sagemaker/).\n",
    "1. Select Notebook instances and choose the name of your notebook instance.\n",
    "1. Under Permissions and encryption select the role ARN to view the role on the IAM console.\n",
    "1. Copy and save the IAM role ARN for later use.\n",
    "1. Choose Attach policies and search for AWSStepFunctionsFullAccess.\n",
    "1. Select the check box next to AWSStepFunctionsFullAccess and choose Attach policy.\n",
    "\n",
    "Next, let's create an execution role in IAM for Step Functions.\n",
    "\n",
    "### Create an Execution Role for Step Functions\n",
    "\n",
    "Your Step Functions workflow requires an IAM role to interact with other services in your AWS environment.\n",
    "\n",
    "1. Go to the IAM console.\n",
    "1. Select Roles and then Create role.\n",
    "1. Under Choose the service that will use this role select Step Functions.\n",
    "1. Choose Next until you can enter a Role name.\n",
    "1. Enter a name such as StepFunctionsWorkflowExecutionRole and then select Create role.\n",
    "\n",
    "Next, create and attach a policy to the role you created. As a best practice, the following steps will attach a policy that only provides access to the specific resources and actions needed for this solution.\n",
    "\n",
    "1. Under the Permissions tab, click Attach policies and then Create policy.\n",
    "1. Enter the following in the JSON tab:\n",
    "    ```json\n",
    "    {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": \"iam:PassRole\",\n",
    "                \"Resource\": \"NOTEBOOK_ROLE_ARN\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"sagemaker:CreateModel\",\n",
    "                    \"sagemaker:DeleteEndpointConfig\",\n",
    "                    \"sagemaker:DescribeTrainingJob\",\n",
    "                    \"sagemaker:CreateEndpoint\",\n",
    "                    \"sagemaker:StopTrainingJob\",\n",
    "                    \"sagemaker:CreateTrainingJob\",\n",
    "                    \"sagemaker:UpdateEndpoint\",\n",
    "                    \"sagemaker:CreateEndpointConfig\",\n",
    "                    \"sagemaker:DeleteEndpoint\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    \"arn:aws:sagemaker:*:*:*\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"events:DescribeRule\",\n",
    "                    \"events:PutRule\",\n",
    "                    \"events:PutTargets\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTrainingJobsRule\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"lambda:InvokeFunction\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    \"arn:aws:lambda:*:*:function:*QueryTraining*\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    ```\n",
    "1. Replace NOTEBOOK_ROLE_ARN with the ARN for your notebook that you created in the previous step.\n",
    "1. Choose Review policy and give the policy a name such as StepFunctionsWorkflowExecutionPolicy.\n",
    "1. Choose Create policy.\n",
    "1. Select Roles and search for your StepFunctionsWorkflowExecutionRole role.\n",
    "1. Under the Permissions tab, click Attach policies.\n",
    "1. Search for your newly created StepFunctionsWorkflowExecutionPolicy policy and select the check box next to it.\n",
    "1. Choose Attach policy. You will then be redirected to the details page for the role.\n",
    "1. Copy the StepFunctionsWorkflowExecutionRole Role ARN at the top of the Summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the permissions that the Step Function Workflow will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste the StepFunctionsWorkflowExecutionRole ARN from above\n",
    "workflow_execution_role = ''\n",
    "\n",
    "# SageMaker Execution Role\n",
    "# You can use sagemaker.get_execution_role() if running inside sagemaker's notebook instance\n",
    "sagemaker_execution_role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the locations where the training data will be placed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prefix = 'train'\n",
    "test_prefix = 'test'\n",
    "key = 'recordio-pb-data'\n",
    "\n",
    "train_data = 's3://{}/{}/'.format(data_bucket, train_prefix)\n",
    "test_data = 's3://{}/{}/'.format(data_bucket, test_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the API for your Step Function Workflow\n",
    "This is the API that controls the inputs of your State Machine.\n",
    "\n",
    "* TrainingJobName = The name of your SageMaker Training Job\n",
    "* LambdaDataPrep = The name of the Lambda function responsible for data prep\n",
    "* LambdaDataSplit = The name of the Lambda function responsible for splitting the data\n",
    "* ModelName = The name of the model we'll be training\n",
    "* EndpointName = The name of the Endpoint (unused)\n",
    "* LambdaQueryStatus = The name of the Lambda function responsible for querying the model accuracy\n",
    "* data_bucket = The name of the S3 bucket where all of the data for the workflow will be persisted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker expects unique names for each job, model and endpoint. \n",
    "# If these names are not unique the execution will fail.\n",
    "execution_input = ExecutionInput(schema={\n",
    "    'TrainingJobName': str,\n",
    "    'LambdaDataPrep': str,\n",
    "    'LambdaDataSplit': str,\n",
    "    'ModelName': str,\n",
    "    'EndpointName': str,\n",
    "    'LambdaQueryStatus': str,\n",
    "    'data_bucket': str\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Estimator for training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "\n",
    "def estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=None):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, \n",
    "    and return a deployed predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    linear = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"linear-learner\"),\n",
    "        get_execution_role(),\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.m5.2xlarge',\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session())\n",
    "    linear.set_hyperparameters(**hyperparams)\n",
    "    return linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# TODO: Update hard coded feature_dim and mini_batch_size params to be dynamic again (refer to old code)\n",
    "hyperparams = {\n",
    "    'feature_dim': 6,\n",
    "    'mini_batch_size': 17995,\n",
    "    'predictor_type': 'binary_classifier' \n",
    "}\n",
    "\n",
    "output_path = 's3://' + data_bucket\n",
    "linear_estimator = estimator_from_hyperparams(train_data, hyperparams, output_path, \n",
    "                                                   s3_test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Step Function Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_data = 's3://{}/{}/{}'.format(data_bucket, train_prefix, key)\n",
    "s3_test_data = 's3://{}/{}/{}'.format(data_bucket, test_prefix, key)\n",
    "\n",
    "training_step = steps.TrainingStep(\n",
    "    'Model Training', \n",
    "    estimator=linear_estimator,\n",
    "    data={\n",
    "        'train': s3_train_data,\n",
    "        'test': s3_test_data\n",
    "    },\n",
    "    job_name=execution_input['TrainingJobName'],\n",
    "    wait_for_completion=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Step Function Model Save Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = steps.ModelStep(\n",
    "    'Save Model',\n",
    "    model=training_step.get_expected_model(),\n",
    "    model_name=execution_input['ModelName'],\n",
    "    result_path='$.ModelStepResults'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Lambda Step Function Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query lambda\n",
    "lambda_step_query = steps.compute.LambdaStep(\n",
    "    'Query Training Results',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['LambdaQueryStatus'],\n",
    "        'Payload':{\n",
    "            \"TrainingJobName.$\": '$.TrainingJobName'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# data prep lambda\n",
    "lambda_step_prep = steps.compute.LambdaStep(\n",
    "    'Prep Data in Athena',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['LambdaDataPrep'],\n",
    "        'Payload':{\n",
    "            \"data_bucket\": execution_input['data_bucket']\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# data split lambda\n",
    "lambda_step_split = steps.compute.LambdaStep(\n",
    "    'Split Data',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['LambdaDataSplit'],\n",
    "        'Payload': lambda_step_prep.output()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy_step = steps.states.Choice(\n",
    "    'Accuracy > 90%'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need some code here to move the trained model to the \"production\" or \"deploy\" area for lambda to pick up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_step = steps.states.Fail(\n",
    "    'Model Accuracy Too Low',\n",
    "    comment='Validation accuracy lower than threshold'\n",
    ")\n",
    "\n",
    "end_step = steps.states.Pass('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_rule = steps.choice_rule.ChoiceRule.NumericLessThan(variable=lambda_step_query.output()['Payload']['trainingMetrics'][0]['Value'], value=.1)\n",
    "\n",
    "check_accuracy_step.add_choice(rule=threshold_rule, next_step=end_step)\n",
    "check_accuracy_step.default_choice(next_step=fail_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_definition = steps.Chain([\n",
    "    lambda_step_prep,\n",
    "    lambda_step_split,\n",
    "    training_step,\n",
    "    model_step,\n",
    "    lambda_step_query,\n",
    "    check_accuracy_step\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_name = 'MLOpsRetrainingWorkflow_{}'.format(id)\n",
    "workflow = Workflow(\n",
    "    name=workflow_name,\n",
    "    definition=workflow_definition,\n",
    "    role=workflow_execution_role,\n",
    "    execution_input=execution_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.render_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the workflow in AWS Step Functions with `create`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_machines = Workflow.list_workflows()\n",
    "workflow_exist = False\n",
    "existing_workflow_obj = {}\n",
    "for machine in state_machines:\n",
    "    if machine[\"name\"] == workflow_name:\n",
    "        workflow_exist = True\n",
    "        existing_workflow_obj = machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if workflow_exist:\n",
    "    workflow = Workflow.attach(existing_workflow_obj[\"stateMachineArn\"])\n",
    "    print(workflow_name + \" already exist.  Updating the workflow definition.\")\n",
    "    workflow.update(definition=workflow_definition,role=workflow_execution_role)\n",
    "    # small delay for step functions to pick up the new definitions before execution\n",
    "    time.sleep(2)\n",
    "else:\n",
    "    print(workflow_name + \" does not exist.  Creating it with the specified workflow definition.\")\n",
    "    workflow.create()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the workflow with `execute`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a58fdf0d-32fb-4690-add3-433cc721773d"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "execution = workflow.execute(\n",
    "    inputs={\n",
    "        'TrainingJobName': 'linear-learner-{}'.format(id), # Each Sagemaker Job requires a unique name,\n",
    "        'ModelName': 'UnicornWeatherImpact-{}'.format(id), # Each Model requires a unique name,\n",
    "        'EndpointName': 'UnicornStatus', # Each Endpoint requires a unique name\n",
    "        'LambdaQueryStatus': '',\n",
    "        'LambdaDataPrep': '',\n",
    "        'LambdaDataSplit': '',\n",
    "        'data_bucket': data_bucket\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
